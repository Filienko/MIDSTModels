{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ5My1aIV2Tv"
   },
   "source": [
    "# Membership Inference over Diffusion-models-based Synthetic Tabular Data (MIDST) Challenge @ SaTML 2025.\n",
    "\n",
    "## White Box Single Table Competition\n",
    "Welcome to the MIDST challenge!\n",
    "\n",
    "The MIDST challenge is a multi-track competition aiming to quantitatively evaluate the privacy of synthetic tabular data generated by diffusion models, with a specific focus on its resistance to membership inference attacks (MIAs).\n",
    "\n",
    "This competition focuses on White Box MIA on tabular diffusion models trained on a single table transaction dataset. The schema of the transaction dataset is as follows:\n",
    "| trans_id | account_id | trans_date | trans_type | operation | amount  | balance  | k_symbol | bank | account |\n",
    "|----------|------------|------------|------------|-----------|---------|----------|----------|------|---------|\n",
    "| integer  | integer    | integer    | integer    | integer   | float   | float    | integer  | integer | integer |\n",
    "\n",
    "MIA will be performed over two state-of-the-art methods [TabSyn](https://arxiv.org/pdf/2310.09656) and [TabDDPM](https://arxiv.org/pdf/2209.15421). A collection of TabSyn and TabDDPM models will be trained on random subsets of the transaction dataset. The goal is to create an approach (MIA) that can distinguish between samples used to train a model (train data) and other data randomly sampled from the transaction dataset (holdout data) given the model and it's output synthetic data. The `final` set includes 20 models, each with its own set of challenge points (ie train and holdout data), to evaluate solutions on. To facilitate designing an attack, 30 `train` models are provided with comprehensive information about the model, training data and output synthetic data. Additionally, 20 `dev` models are provided to assist in evaluating the effectiveness of attacks prior to making a final submission to the `final` set. Participants can choose to perform MIA over one of or both TabSyn and TabDDPM. In the case of both, the attack that obtains the highest score will be used to rank the submission. A high level summary of the competition is below:\n",
    "![wbox_diagram_final](https://github.com/user-attachments/assets/2ebb5eed-a6e3-433a-8769-4310b7fbc822)\n",
    "\n",
    "This notebook will walk you through the process of creating and packaging a submission to the white box single table challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksush\\attacks\\MIDSTModels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import load_multi_table\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import tabddpm_whitebox_load_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w_id_df = pd.read_csv(\"tabddpm_white_box/train/tabddpm_1/train_with_id.csv\")\n",
    "train_df = train_w_id_df.iloc[:, 2:]\n",
    "train_df.to_csv(\"tabddpm_white_box/train/tabddpm_1/train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found, loading...\n",
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/projects/aieng/midst_competition/data/tabddpm/tabddpm_1\",\n",
      "        \"exp_name\": \"train_1\",\n",
      "        \"workspace_dir\": \"/projects/aieng/midst_competition/data/tabddpm/tabddpm_1/workspace\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"/projects/aieng/midst_competition/data/tabddpm/tabddpm_1\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n",
      "Table name: trans, Total dataframe shape: (20000, 8), Numerical data shape: (20000, 4), Categorical data shape: (20000, 4)\n",
      "==================== We show the keys of the tables dictionary below ====================\n",
      "['trans']\n",
      "==================== We show the clustering parameters below ====================\n",
      "parent_scale: 1.0\n",
      "num_clusters: 50\n",
      "clustering_method: both\n",
      "\n",
      "Clustering checkpoint found, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning:\n",
      "\n",
      "Trying to unpickle estimator LabelEncoder from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning:\n",
      "\n",
      "Trying to unpickle estimator QuantileTransformer from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "c:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:221: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "c:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:221: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "c:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:221: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "c:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:221: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"tabddpm_white_box/train/tabddpm_1\"\n",
    "model = tabddpm_whitebox_load_pretrained(path)\n",
    "\n",
    "# Load config\n",
    "config_path = \"tabddpm_white_box/train/tabddpm_1/trans.json\"\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)\n",
    "\n",
    "# Load  dataset\n",
    "# In this step, we load the dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "\n",
    "tables, relation_order, dataset_meta = load_multi_table(path)\n",
    "\n",
    "# Tables is a dictionary of the multi-table dataset\n",
    "print(\n",
    "    \"{} We show the keys of the tables dictionary below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "print(list(tables.keys()))\n",
    "\n",
    "# Display important clustering parameters\n",
    "params_clustering = configs[\"clustering\"]\n",
    "print(\"{} We show the clustering parameters below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "for key, val in params_clustering.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")\n",
    "\n",
    "# Clustering on the multi-table dataset\n",
    "tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "    tables, relation_order, save_dir, configs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating None -> trans\n",
      "Sample size: 20000\n",
      "Sample timestep 1970\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate synthetic data from scratch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cleaned_tables, synthesizing_time_spent, matching_time_spent \u001b[38;5;241m=\u001b[39m \u001b[43mclava_synthesizing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelation_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_group_lengths_prob_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebug\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msample_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\complex_pipeline.py:146\u001b[0m, in \u001b[0;36mclava_synthesizing\u001b[1;34m(tables, relation_order, save_dir, all_group_lengths_prob_dicts, models, configs, sample_scale)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample size: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mint\u001b[39m(sample_scale \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(df_without_id))))\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     _, child_generated \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_diffusion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_without_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdf_info\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiffusion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel_encoders\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_without_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_params\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mT_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mT_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     child_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(child_generated)))\n\u001b[0;32m    158\u001b[0m     generated_final_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    159\u001b[0m         [np\u001b[38;5;241m.\u001b[39marray(child_keys)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), child_generated\u001b[38;5;241m.\u001b[39mto_numpy()],\n\u001b[0;32m    160\u001b[0m         axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    161\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:449\u001b[0m, in \u001b[0;36msample_from_diffusion\u001b[1;34m(df, df_info, diffusion, dataset, label_encoders, sample_size, model_params, T_dict, sample_batch_size)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# print(d_in)\u001b[39;00m\n\u001b[0;32m    446\u001b[0m _, empirical_class_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(\n\u001b[0;32m    447\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(dataset\u001b[38;5;241m.\u001b[39my[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]), return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    448\u001b[0m )\n\u001b[1;32m--> 449\u001b[0m x_gen, y_gen \u001b[38;5;241m=\u001b[39m \u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_all\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempirical_class_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m X_gen, y_gen \u001b[38;5;241m=\u001b[39m x_gen\u001b[38;5;241m.\u001b[39mnumpy(), y_gen\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    453\u001b[0m num_numerical_features_sample \u001b[38;5;241m=\u001b[39m num_numerical_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[0;32m    454\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mis_regression \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_y_cond\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    455\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\gaussian_multinomial_diffsuion.py:1112\u001b[0m, in \u001b[0;36mGaussianMultinomialDiffusion.sample_all\u001b[1;34m(self, num_samples, batch_size, y_dist, ddim, model_kwargs, cond_fn)\u001b[0m\n\u001b[0;32m   1110\u001b[0m num_generated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m num_generated \u001b[38;5;241m<\u001b[39m num_samples:\n\u001b[1;32m-> 1112\u001b[0m     sample, out_dict \u001b[38;5;241m=\u001b[39m \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_dist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond_fn\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1115\u001b[0m     mask_nan \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39many(sample\u001b[38;5;241m.\u001b[39misnan(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1116\u001b[0m     sample \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m~\u001b[39mmask_nan]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\gaussian_multinomial_diffsuion.py:1067\u001b[0m, in \u001b[0;36mGaussianMultinomialDiffusion.sample\u001b[1;34m(self, num_samples, y_dist, model_kwargs, cond_fn)\u001b[0m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample timestep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1066\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((b,), i, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m-> 1067\u001b[0m model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_denoise_fn(\n\u001b[0;32m   1068\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat([z_norm, log_z], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(), t, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mout_dict\n\u001b[0;32m   1069\u001b[0m )\n\u001b[0;32m   1070\u001b[0m model_out_num \u001b[38;5;241m=\u001b[39m model_out[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_numerical_features]\n\u001b[0;32m   1071\u001b[0m model_out_cat \u001b[38;5;241m=\u001b[39m model_out[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_numerical_features :]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\modules.py:469\u001b[0m, in \u001b[0;36mMLPDiffusion.forward\u001b[1;34m(self, x, timesteps, y)\u001b[0m\n\u001b[0;32m    467\u001b[0m     emb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_emb(y))\n\u001b[0;32m    468\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x) \u001b[38;5;241m+\u001b[39m emb\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\modules.py:249\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    247\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 249\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\modules.py:166\u001b[0m, in \u001b[0;36mMLP.Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate synthetic data from scratch\n",
    "cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "    tables,\n",
    "    relation_order,\n",
    "    save_dir,\n",
    "    all_group_lengths_prob_dicts,\n",
    "    model,\n",
    "    configs,\n",
    "    # sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    "    sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ5My1aIV2Tv"
   },
   "source": [
    "## Package Imports and Evironment Setup\n",
    "\n",
    "Ensure that you have installed the proper dependenices to run the notebook. The environment installation instructions are available [here](https://github.com/VectorInstitute/MIDSTModels/tree/main/starter_kits). Now that we have verfied we have the proper packages installed, lets import them and define global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MB3iIVMTFYyB"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from data import get_challenge_points, get_challenge_labels\n",
    "from metrics import get_tpr_at_fpr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksush\\attacks\\MIDSTModels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MB3iIVMTFYyB"
   },
   "outputs": [],
   "source": [
    "TABDDPM_DATA_DIR = \"tabddpm_white_box\"\n",
    "TABSYN_DATA_DIR = \"tabsyn_white_box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_FLAGS():\n",
    "    def FLAGS(x): return x\n",
    "    FLAGS.T = 5000\n",
    "    FLAGS.ch = 128\n",
    "    FLAGS.ch_mult = [1, 2, 2, 2]\n",
    "    FLAGS.attn = [1]\n",
    "    FLAGS.num_res_blocks = 2\n",
    "    FLAGS.dropout = 0.1\n",
    "    FLAGS.beta_1 = 0.001\n",
    "    FLAGS.beta_T = 0.2\n",
    "\n",
    "    return FLAGS\n",
    "\n",
    "FLAGS = get_FLAGS()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_df = pd.DataFrame(columns=['average_distance', 'label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "from typing import Dict, Type\n",
    "import seaborn as sns\n",
    "from torch.nn.functional import normalize\n",
    "import midst_models.attack.components as components\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EpsGetter(components.EpsGetter):\n",
    "    def __call__(self, xt: torch.Tensor, condition: torch.Tensor = None, noise_level=None, t: int = None) -> torch.Tensor:\n",
    "        # Access the diffusion model from the dictionary structure\n",
    "        model = self.model[(None, 'trans')]['diffusion']\n",
    "\n",
    "        t = torch.ones([xt.shape[0]], device=xt.device).long() * t\n",
    "        return model._denoise_fn(xt, timesteps=t)\n",
    "\n",
    "ATTACKERS: Dict[str, Type[components.DDIMAttacker]] = {\n",
    "    \"PIA\": components.PIA,\n",
    "    \"PIAN\": components.PIAN,\n",
    "}\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def attack(base_dir, attacker_name=\"PIA\", attack_num=30, interval=10, lp=3):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # logger.addHandler(RichHandler())\n",
    "\n",
    "    logger.info(\"loading the attacked model...\")\n",
    "\n",
    "    # Initialize attacker\n",
    "    phases = [\"train\"]\n",
    "    # phases = [\"dev\", \"final\"]\n",
    "    \n",
    "    logger.info(\"attack start...\")\n",
    "    for phase in phases:\n",
    "        root = os.path.join(base_dir, phase)\n",
    "        for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            if \"30\" not in str(path):\n",
    "                continue\n",
    "\n",
    "            model = tabddpm_whitebox_load_pretrained(path)\n",
    "            attacker = ATTACKERS[attacker_name](\n",
    "                torch.from_numpy(np.linspace(FLAGS.beta_1, FLAGS.beta_T, FLAGS.T)).to(DEVICE), interval, attack_num, EpsGetter(model), lp=lp)\n",
    "\n",
    "            challenge_points = get_challenge_points(path)\n",
    "            challenge_labels = get_challenge_labels(path)\n",
    "            raw_predictions = torch.stack([attacker(cp.to(DEVICE).float()) for cp in challenge_points])\n",
    "            raw_predictions_means = raw_predictions.mean(dim=1).cpu().detach().numpy()\n",
    "            challenge_labels_np = challenge_labels.values.squeeze()\n",
    "\n",
    "            non_member_distances = raw_predictions_means[challenge_labels_np == 0]\n",
    "            member_distances = raw_predictions_means[challenge_labels_np == 1]\n",
    "\n",
    "            # Create figure with three subplots\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15), height_ratios=[1, 1, 1.5])\n",
    "            \n",
    "            # Plot 1: Full distribution\n",
    "            bins = np.linspace(min(raw_predictions_means), max(raw_predictions_means), 30)\n",
    "            ax1.hist(non_member_distances, bins=bins, alpha=0.6, \n",
    "                    label=f'Non-member (n={len(non_member_distances)})', \n",
    "                    color='blue')\n",
    "            ax1.hist(member_distances, bins=bins, alpha=0.6,\n",
    "                    label=f'Member (n={len(member_distances)})', \n",
    "                    color='red')\n",
    "            \n",
    "            ax1.set_xlabel('Distance')\n",
    "            ax1.set_ylabel('Count')\n",
    "            ax1.set_title('Full Distribution of Distances')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # First zoom: Find the densest region\n",
    "            all_data = np.concatenate([member_distances, non_member_distances])\n",
    "            Q1 = np.percentile(all_data, 25)\n",
    "            Q3 = np.percentile(all_data, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 0.5 * IQR\n",
    "            upper_bound = Q3 + 0.5 * IQR\n",
    "\n",
    "            # Filter data for first zoom\n",
    "            non_member_filtered = non_member_distances[\n",
    "                (non_member_distances >= lower_bound) & \n",
    "                (non_member_distances <= upper_bound)\n",
    "            ]\n",
    "            member_filtered = member_distances[\n",
    "                (member_distances >= lower_bound) & \n",
    "                (member_distances <= upper_bound)\n",
    "            ]\n",
    "\n",
    "            # Plot 2: First zoom level\n",
    "            detailed_bins = np.linspace(lower_bound, upper_bound, 50)\n",
    "            ax2.hist(non_member_filtered, bins=detailed_bins, alpha=0.6,\n",
    "                    label=f'Non-member (n={len(non_member_filtered)})', \n",
    "                    color='blue')\n",
    "            ax2.hist(member_filtered, bins=detailed_bins, alpha=0.6,\n",
    "                    label=f'Member (n={len(member_filtered)})', \n",
    "                    color='red')\n",
    "            \n",
    "            ax2.set_xlabel('Distance')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.set_title('First Zoom Level\\n'\n",
    "                        f'(Range: {lower_bound:.2f} to {upper_bound:.2f})')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            # Second zoom: Find the even denser region\n",
    "            filtered_data = np.concatenate([member_filtered, non_member_filtered])\n",
    "            Q1_filtered = np.percentile(filtered_data, 25)\n",
    "            Q3_filtered = np.percentile(filtered_data, 75)\n",
    "            IQR_filtered = Q3_filtered - Q1_filtered\n",
    "            lower_bound_filtered = Q1_filtered - 0.25 * IQR_filtered  # Using tighter bounds\n",
    "            upper_bound_filtered = Q3_filtered + 0.25 * IQR_filtered\n",
    "\n",
    "            # Filter data for second zoom\n",
    "            non_member_filtered_2 = non_member_filtered[\n",
    "                (non_member_filtered >= lower_bound_filtered) & \n",
    "                (non_member_filtered <= upper_bound_filtered)\n",
    "            ]\n",
    "            member_filtered_2 = member_filtered[\n",
    "                (member_filtered >= lower_bound_filtered) & \n",
    "                (member_filtered <= upper_bound_filtered)\n",
    "            ]\n",
    "\n",
    "            # Plot 3: Second zoom level with very fine bins\n",
    "            very_detailed_bins = np.linspace(lower_bound_filtered, upper_bound_filtered, 100)\n",
    "            ax3.hist(non_member_filtered_2, bins=very_detailed_bins, alpha=0.6,\n",
    "                    label=f'Non-member (n={len(non_member_filtered_2)})', \n",
    "                    color='blue')\n",
    "            ax3.hist(member_filtered_2, bins=very_detailed_bins, alpha=0.6,\n",
    "                    label=f'Member (n={len(member_filtered_2)})', \n",
    "                    color='red')\n",
    "            \n",
    "            ax3.set_xlabel('Distance')\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.set_title('Second Zoom Level (Finest Detail)\\n'\n",
    "                        f'(Range: {lower_bound_filtered:.2f} to {upper_bound_filtered:.2f})')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "\n",
    "            # Add statistical information for the finest zoom level\n",
    "            stats_text = (\n",
    "                f'Dense Region Stats:\\n'\n",
    "                f'Non-member:\\n'\n",
    "                f'  Mean: {np.mean(non_member_filtered_2):.3f}\\n'\n",
    "                f'  Std: {np.std(non_member_filtered_2):.3f}\\n'\n",
    "                f'Member:\\n'\n",
    "                f'  Mean: {np.mean(member_filtered_2):.3f}\\n'\n",
    "                f'  Std: {np.std(member_filtered_2):.3f}'\n",
    "            )\n",
    "            ax3.text(0.02, 0.98, stats_text, \n",
    "                    transform=ax3.transAxes,\n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(path, f\"distance_distribution_T={interval}_lp={lp}.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            # Save detailed statistics for all zoom levels\n",
    "            stats_summary = pd.DataFrame({\n",
    "                'Metric': ['Full Mean', 'Full Std', \n",
    "                        'First Zoom Mean', 'First Zoom Std',\n",
    "                        'Second Zoom Mean', 'Second Zoom Std',\n",
    "                        'Points in Densest Region', 'Total Points'],\n",
    "                'Member': [\n",
    "                    member_distances.mean(),\n",
    "                    member_distances.std(),\n",
    "                    member_filtered.mean(),\n",
    "                    member_filtered.std(),\n",
    "                    member_filtered_2.mean(),\n",
    "                    member_filtered_2.std(),\n",
    "                    len(member_filtered_2),\n",
    "                    len(member_distances)\n",
    "                ],\n",
    "                'Non-member': [\n",
    "                    non_member_distances.mean(),\n",
    "                    non_member_distances.std(),\n",
    "                    non_member_filtered.mean(),\n",
    "                    non_member_filtered.std(),\n",
    "                    non_member_filtered_2.mean(),\n",
    "                    non_member_filtered_2.std(),\n",
    "                    len(non_member_filtered_2),\n",
    "                    len(non_member_distances)\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            stats_summary.to_csv(os.path.join(path, f\"detailed_statistics_T={interval}_lp={lp}.csv\"), index=False)\n",
    "\n",
    "            # Continue with the original prediction code\n",
    "            normalized_preds = []\n",
    "            for pred_batch in raw_predictions:\n",
    "                binary_preds = (pred_batch <= 1000000).float()\n",
    "                normalized_preds.append(binary_preds)\n",
    "\n",
    "            final_predictions = torch.stack(normalized_preds)\n",
    "            predictions_cpu = final_predictions.cpu().detach().numpy()\n",
    "\n",
    "            with open(os.path.join(path, \"prediction.csv\"), mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                for value in predictions_cpu.squeeze():\n",
    "                    writer.writerow([value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint found, loading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning:\n",
      "\n",
      "Trying to unpickle estimator LabelEncoder from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning:\n",
      "\n",
      "Trying to unpickle estimator QuantileTransformer from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\numpy\\_core\\_methods.py:188: RuntimeWarning:\n",
      "\n",
      "overflow encountered in multiply\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attack(base_dir=\"tabddpm_white_box\",\n",
    "        attacker_name=\"PIAN\",\n",
    "        attack_num=3,\n",
    "        interval=200,\n",
    "        lp=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGhGsrlPV2Ty"
   },
   "source": [
    "## Scoring\n",
    "\n",
    "Let's see how the attack does on `train`, for which we have the ground truth.\n",
    "When preparing a submission, you can use part of `train` to develop an attack and a held-out part to evaluate your attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-UN3zfuPV2Ty"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "def safe_load_data(filepath: str, is_prediction: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safely load data from CSV files with error handling and debugging.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        is_prediction: Whether this is a prediction file (True) or label file (False)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Loaded and validated data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_prediction:\n",
    "            # Read the file line by line and parse each array\n",
    "            predictions = []\n",
    "            with open(filepath, 'r') as f:\n",
    "                for line in f:\n",
    "                    # Remove brackets and split by spaces\n",
    "                    clean_line = line.strip().replace('[', '').replace(']', '')\n",
    "                    # Convert space-separated strings to floats\n",
    "                    row = np.array([float(x) for x in clean_line.split()])\n",
    "                    predictions.append(row)\n",
    "            return np.array(predictions)\n",
    "        else:\n",
    "            # For label files, skip header and use numpy\n",
    "            return np.loadtxt(filepath, skiprows=1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {str(e)}\")\n",
    "        print(f\"File contents (first few lines):\")\n",
    "        with open(filepath, 'r') as f:\n",
    "            print(f.read(500))\n",
    "        raise\n",
    "\n",
    "def get_tpr_at_fpr(y_true: np.ndarray, y_score: np.ndarray, target_fpr: float = 0.1) -> float:\n",
    "    \"\"\"Calculate TPR at a specific FPR threshold.\"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(y_true, np.ndarray) or not isinstance(y_score, np.ndarray):\n",
    "        raise TypeError(\"Inputs must be numpy arrays\")\n",
    "    if y_true.shape != y_score.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true {y_true.shape} != y_score {y_score.shape}\")\n",
    "    \n",
    "    # Sort scores and corresponding truth values\n",
    "    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n",
    "    y_score = y_score[desc_score_indices]\n",
    "    y_true = y_true[desc_score_indices]\n",
    "    \n",
    "    n_neg = np.sum(y_true == 0)\n",
    "    n_pos = np.sum(y_true == 1)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if n_neg == 0 or n_pos == 0:\n",
    "        print(\"Warning: Found no positive or negative samples\")\n",
    "        return 0.0\n",
    "        \n",
    "    tpr = np.cumsum(y_true) / n_pos\n",
    "    fpr = np.cumsum(1 - y_true) / n_neg\n",
    "    \n",
    "    for i in range(len(fpr)):\n",
    "        if fpr[i] >= target_fpr:\n",
    "            return tpr[i]\n",
    "    return 1.0\n",
    "\n",
    "def evaluate_membership_inference(base_dirs: List[str]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate membership inference attack results across multiple directories.\n",
    "    \n",
    "    Args:\n",
    "        base_dirs: List of base directories containing prediction files\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_tpr_at_fpr, best_auc)\n",
    "    \"\"\"\n",
    "    tpr_at_fpr_list = []\n",
    "    auc_list = []\n",
    "    \n",
    "    for base_dir in base_dirs:\n",
    "        predictions = []\n",
    "        solutions = []\n",
    "        root = os.path.join(base_dir, \"train\")\n",
    "        \n",
    "        if not os.path.exists(root):\n",
    "            print(f\"Warning: Directory not found: {root}\")\n",
    "            continue\n",
    "            \n",
    "        model_folders = sorted(os.listdir(root), key=lambda d: int(d.split('_')[1]))\n",
    "        if not model_folders:\n",
    "            print(f\"Warning: No model folders found in {root}\")\n",
    "            continue\n",
    "        \n",
    "        # Load and process all predictions and solutions\n",
    "        for model_folder in model_folders:\n",
    "            path = os.path.join(root, model_folder)\n",
    "            pred_path = os.path.join(path, \"prediction.csv\")\n",
    "            label_path = os.path.join(path, \"challenge_label.csv\")\n",
    "            \n",
    "            if not (os.path.exists(pred_path) and os.path.exists(label_path)):\n",
    "                print(f\"Warning: Missing files in {path}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load predictions\n",
    "                pred = safe_load_data(pred_path, is_prediction=True)\n",
    "                predictions.append(pred)\n",
    "                \n",
    "                # Load ground truth\n",
    "                solution = safe_load_data(label_path, is_prediction=False)\n",
    "                if solution.ndim == 0:\n",
    "                    solution = solution.reshape(1)\n",
    "                solutions.append(solution)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing folder {model_folder}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not predictions or not solutions:\n",
    "            print(f\"Warning: No valid data found in {base_dir}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Concatenate all predictions and solutions\n",
    "            predictions = np.concatenate(predictions)\n",
    "            solutions = np.concatenate(solutions)\n",
    "            \n",
    "            print(f\"\\nData shapes for {os.path.basename(base_dir)}:\")\n",
    "            print(f\"Predictions shape: {predictions.shape}\")\n",
    "            print(f\"Solutions shape: {solutions.shape}\")\n",
    "            \n",
    "            # Calculate metrics for each attacker's predictions\n",
    "            num_attackers = predictions.shape[1]\n",
    "            for attacker_idx in range(num_attackers):\n",
    "                attacker_preds = predictions[:, attacker_idx]\n",
    "                \n",
    "                # Basic data validation\n",
    "                if np.any(np.isnan(attacker_preds)):\n",
    "                    print(f\"Warning: NaN values found in predictions for attacker {attacker_idx}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate metrics\n",
    "                tpr_at_fpr = get_tpr_at_fpr(solutions, attacker_preds)\n",
    "                tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "                \n",
    "                try:\n",
    "                    auc = roc_auc_score(solutions, attacker_preds)\n",
    "                    auc_list.append(auc)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Could not calculate AUC for attacker {attacker_idx}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"{os.path.basename(base_dir)} Attacker {attacker_idx + 1}:\")\n",
    "                print(f\"  TPR at FPR==10%: {tpr_at_fpr:.4f}\")\n",
    "                print(f\"  AUC: {auc:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {base_dir}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Get best scores\n",
    "    final_tpr_at_fpr = max(tpr_at_fpr_list) if tpr_at_fpr_list else 0.0\n",
    "    final_auc = max(auc_list) if auc_list else 0.0\n",
    "    \n",
    "    print(f\"\\nBest scores across all attackers:\")\n",
    "    print(f\"Final Train Attack TPR at FPR==10%: {final_tpr_at_fpr:.4f}\")\n",
    "    print(f\"Final Train Attack AUC: {final_auc:.4f}\")\n",
    "    \n",
    "    return final_tpr_at_fpr, final_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data shapes for tabddpm_white_box:\n",
      "Predictions shape: (6000, 3)\n",
      "Solutions shape: (6000,)\n",
      "tabddpm_white_box Attacker 1:\n",
      "  TPR at FPR==10%: 0.1097\n",
      "  AUC: 0.5030\n",
      "tabddpm_white_box Attacker 2:\n",
      "  TPR at FPR==10%: 0.1090\n",
      "  AUC: 0.5015\n",
      "tabddpm_white_box Attacker 3:\n",
      "  TPR at FPR==10%: 0.1040\n",
      "  AUC: 0.4975\n",
      "\n",
      "Best scores across all attackers:\n",
      "Final Train Attack TPR at FPR==10%: 0.1097\n",
      "Final Train Attack AUC: 0.5030\n"
     ]
    }
   ],
   "source": [
    "base_dirs = [TABDDPM_DATA_DIR]\n",
    "final_tpr, final_auc = evaluate_membership_inference(base_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9LZ-EhfV2Ty"
   },
   "source": [
    "## Packaging the submission\n",
    "\n",
    "Now we can store the predictions into a zip file, which you can submit to CodaBench. Importantly, we create a single zip file for dev and final. The structure of the submission is as follows:\n",
    "\n",
    "```\n",
    "└── root_folder\n",
    "    ├── tabsyn_white_box\n",
    "    │   ├── dev\n",
    "    │   │   └── tabsyn_#\n",
    "    │   │       └── prediction.csv\n",
    "    │   └── final\n",
    "    │       └── tabsyn_#\n",
    "    │           └── prediction.csv\n",
    "    └── tabddpm_white_box\n",
    "        ├── dev \n",
    "        │   └── tabddpm_#\n",
    "        │       └── prediction.csv\n",
    "        └── final \n",
    "            └── tabddpm_# \n",
    "                └── prediction.csv\n",
    "```\n",
    "**Note:** The `root_folder` can have any name but it is important all of the subdirectories follow the above structure and naming conventions. \n",
    "\n",
    "If a participant is looking to submit an attack for only one of TabSyn and TabDDPM, they can simply omit the other directory (ie `tabddpm_white_box` or `tabsyn_white_box` from the root_folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ats5N4AoV2Tz"
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_numpy_array(array_str):\n",
    "    \"\"\"Fix NumPy-style array formatting and convert it to a Python list.\"\"\"\n",
    "    array_str = array_str.strip()  # Remove leading/trailing spaces\n",
    "    array_str = re.sub(r'\\s+', ',', array_str)  # Replace spaces with commas\n",
    "    array_str = array_str.replace(\"[,\", \"[\").replace(\",]\", \"]\")  # Fix misplaced commas\n",
    "    return eval(array_str)  # Convert string to list\n",
    "\n",
    "with zipfile.ZipFile(\"white_box_single_table_submission.zip\", 'w') as zipf:\n",
    "    for phase in [\"dev\", \"final\"]:\n",
    "        for base_dir in [TABDDPM_DATA_DIR]:\n",
    "            root = os.path.join(base_dir, phase)\n",
    "            for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):\n",
    "                path = os.path.join(root, model_folder)\n",
    "                if not os.path.isdir(path): \n",
    "                    continue\n",
    "\n",
    "                file = os.path.join(path, \"prediction.csv\")\n",
    "                if os.path.exists(file):\n",
    "                    # Load CSV\n",
    "                    df = pd.read_csv(file, header=None)\n",
    "\n",
    "                    # Convert NumPy-style arrays to proper lists\n",
    "                    df = df[0].apply(lambda x: parse_numpy_array(x) if isinstance(x, str) else x)\n",
    "\n",
    "                    # Compute mean for each row\n",
    "                    df_mean = df.apply(lambda x: np.mean(x) if isinstance(x, list) else x)\n",
    "\n",
    "                    # Save the new CSV\n",
    "                    df_mean.to_csv(file, index=False, header=False)\n",
    "\n",
    "                    # Add to ZIP\n",
    "                    arcname = os.path.relpath(file, os.path.dirname(base_dir))\n",
    "                    zipf.write(file, arcname=arcname)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"`prediction.csv` not found in {path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated white_box_single_table_submission.zip can be directly submitted to the dev phase in the CodaBench UI. Although this submission contains your predictions for both the dev and final set, you will only receive feedback on your predictions for the dev phase. The predictions for the final phase will be evaluated once the competiton ends using the most recent submission to the dev phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
