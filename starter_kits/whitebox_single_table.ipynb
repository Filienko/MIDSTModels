{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ5My1aIV2Tv"
   },
   "source": [
    "# Membership Inference over Diffusion-models-based Synthetic Tabular Data (MIDST) Challenge @ SaTML 2025.\n",
    "\n",
    "## White Box Single Table Competition\n",
    "Welcome to the MIDST challenge!\n",
    "\n",
    "The MIDST challenge is a multi-track competition aiming to quantitatively evaluate the privacy of synthetic tabular data generated by diffusion models, with a specific focus on its resistance to membership inference attacks (MIAs).\n",
    "\n",
    "This competition focuses on White Box MIA on tabular diffusion models trained on a single table transaction dataset. The schema of the transaction dataset is as follows:\n",
    "| trans_id | account_id | trans_date | trans_type | operation | amount  | balance  | k_symbol | bank | account |\n",
    "|----------|------------|------------|------------|-----------|---------|----------|----------|------|---------|\n",
    "| integer  | integer    | integer    | integer    | integer   | float   | float    | integer  | integer | integer |\n",
    "\n",
    "MIA will be performed over two state-of-the-art methods [TabSyn](https://arxiv.org/pdf/2310.09656) and [TabDDPM](https://arxiv.org/pdf/2209.15421). A collection of TabSyn and TabDDPM models will be trained on random subsets of the transaction dataset. The goal is to create an approach (MIA) that can distinguish between samples used to train a model (train data) and other data randomly sampled from the transaction dataset (holdout data) given the model and it's output synthetic data. The `final` set includes 20 models, each with its own set of challenge points (ie train and holdout data), to evaluate solutions on. To facilitate designing an attack, 30 `train` models are provided with comprehensive information about the model, training data and output synthetic data. Additionally, 20 `dev` models are provided to assist in evaluating the effectiveness of attacks prior to making a final submission to the `final` set. Participants can choose to perform MIA over one of or both TabSyn and TabDDPM. In the case of both, the attack that obtains the highest score will be used to rank the submission. A high level summary of the competition is below:\n",
    "![wbox_diagram_final](https://github.com/user-attachments/assets/2ebb5eed-a6e3-433a-8769-4310b7fbc822)\n",
    "\n",
    "This notebook will walk you through the process of creating and packaging a submission to the white box single table challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksush\\attacks\\MIDSTModels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    "    clava_attacking\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import load_multi_table\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import tabddpm_whitebox_load_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Dict, Type\n",
    "import midst_models.attack.components as components\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "class EpsGetter(components.EpsGetter):\n",
    "    def __call__(self, xt: torch.Tensor, condition: torch.Tensor = None, noise_level=None, t: int = None) -> torch.Tensor:\n",
    "        # Access the diffusion model from the dictionary structure\n",
    "        model = self.model\n",
    "\n",
    "        t = torch.ones([xt.shape[0]], device=xt.device).long() * t\n",
    "        return model(xt, timesteps=t)\n",
    "\n",
    "ATTACKERS: Dict[str, Type[components.DDIMAttacker]] = {\n",
    "    \"PIA\": components.PIA,\n",
    "    \"PIAN\": components.PIAN,\n",
    "}\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def get_FLAGS():\n",
    "    def FLAGS(x): return x\n",
    "    FLAGS.T = 5000\n",
    "    FLAGS.ch = 128\n",
    "    FLAGS.ch_mult = [1, 2, 2, 2]\n",
    "    FLAGS.attn = [1]\n",
    "    FLAGS.num_res_blocks = 2\n",
    "    FLAGS.dropout = 0.1\n",
    "    FLAGS.beta_1 = 0.001\n",
    "    FLAGS.beta_T = 0.2\n",
    "\n",
    "    return FLAGS\n",
    "\n",
    "FLAGS = get_FLAGS()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GENERATE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ksush\\attacks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning:\n",
      "\n",
      "using dhist requires you to install the `pickleshare` library.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"c:/Users/ksush/attacks/MIDSTModels/tabddpm_white_box/final/tabddpm_41\"\n",
    "# model = tabddpm_whitebox_load_pretrained(path)\n",
    "\n",
    "# # Load config\n",
    "# config_path = \"c:/Users/ksush/attacks/MIDSTModels/tabddpm_white_box/final/tabddpm_41/trans.json\"\n",
    "# configs, save_dir = load_configs(config_path)\n",
    "# # os.chdir(path)\n",
    "\n",
    "# # Display config\n",
    "# json_str = json.dumps(configs, indent=4)\n",
    "# print(json_str)\n",
    "\n",
    "# # Load  dataset\n",
    "# # In this step, we load the dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "# tables, relation_order, dataset_meta = load_multi_table(path, train_data=\"challenge\")\n",
    "\n",
    "# # Tables is a dictionary of the multi-table dataset\n",
    "# print(\n",
    "#     \"{} We show the keys of the tables dictionary below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    "# )\n",
    "# print(list(tables.keys()))\n",
    "\n",
    "\n",
    "# # Display important clustering parameters\n",
    "# params_clustering = configs[\"clustering\"]\n",
    "# print(\"{} We show the clustering parameters below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "# for key, val in params_clustering.items():\n",
    "#     print(f\"{key}: {val}\")\n",
    "# print(\"\")\n",
    "\n",
    "# # Clustering on the multi-table dataset\n",
    "# tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "#     tables, relation_order, save_dir, configs\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate synthetic data from scratch\n",
    "# cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "#     tables,\n",
    "#     relation_order,\n",
    "#     save_dir,\n",
    "#     all_group_lengths_prob_dicts,\n",
    "#     model,\n",
    "#     configs,\n",
    "#     # sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    "#     sample_scale=1.0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning:\n",
      "\n",
      "Trying to unpickle estimator LabelEncoder from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "C:\\Users\\ksush\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning:\n",
      "\n",
      "Trying to unpickle estimator QuantileTransformer from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "\n",
      "c:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:222: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "c:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:222: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "c:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:222: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n",
      "c:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:222: FutureWarning:\n",
      "\n",
      "Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing folder: C:\\Users\\ksush\\attacks\\MIDSTModels\\tabddpm_white_box\\dev\\tabddpm_31\n",
      "Checkpoint found, loading...\n",
      "\n",
      "===== Configurations =====\n",
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/projects/aieng/midst_competition/data/tabddpm/tabddpm_1\",\n",
      "        \"exp_name\": \"train_1\",\n",
      "        \"workspace_dir\": \"/projects/aieng/midst_competition/data/tabddpm/tabddpm_1/workspace\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"/projects/aieng/midst_competition/data/tabddpm/tabddpm_1\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 200,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 200,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n",
      "Loading challenge.csv\n",
      "YES, Loading challenge.csv\n",
      "Table name: trans, Total dataframe shape: (200, 8), Numerical data shape: (200, 4), Categorical data shape: (200, 4)\n",
      "\n",
      "===== Table Keys =====\n",
      "['trans']\n",
      "\n",
      "===== Clustering Parameters =====\n",
      "parent_scale: 1.0\n",
      "num_clusters: 50\n",
      "clustering_method: both\n",
      "Finished processing C:\\Users\\ksush\\attacks\\MIDSTModels\\tabddpm_white_box\\dev\\tabddpm_31\n",
      "==================================================\n",
      "Training None -> trans model from scratch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m final_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mksush\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mattacks\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMIDSTModels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtabddpm_white_box\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfinal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Process all tabddpm folders in dev and final\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mprocess_tabddpm_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m process_tabddpm_folders(final_base)\n",
      "Cell \u001b[1;32mIn[7], line 53\u001b[0m, in \u001b[0;36mprocess_tabddpm_folders\u001b[1;34m(base_path, attacker_name, interval, attack_num)\u001b[0m\n\u001b[0;32m     49\u001b[0m attacker \u001b[38;5;241m=\u001b[39m ATTACKERS[attacker_name](\n\u001b[0;32m     50\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mlinspace(FLAGS\u001b[38;5;241m.\u001b[39mbeta_1, FLAGS\u001b[38;5;241m.\u001b[39mbeta_T, FLAGS\u001b[38;5;241m.\u001b[39mT))\u001b[38;5;241m.\u001b[39mto(DEVICE), interval, attack_num, EpsGetter(model), \u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, lp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Launch training from scratch\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mclava_attacking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelation_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattacker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# # Generate synthetic data from scratch\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# clava_synthesizing(\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#     tables,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#     attacker=attacker\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished performing the MIA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\complex_pipeline.py:98\u001b[0m, in \u001b[0;36mclava_attacking\u001b[1;34m(tables, relation_order, save_dir, configs, attacker, model)\u001b[0m\n\u001b[0;32m     96\u001b[0m id_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_with_cluster\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col]\n\u001b[0;32m     97\u001b[0m df_without_id \u001b[38;5;241m=\u001b[39m df_with_cluster\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mid_cols)\n\u001b[1;32m---> 98\u001b[0m child_attacking(\n\u001b[0;32m     99\u001b[0m     df_without_id, tables[child][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdomain\u001b[39m\u001b[38;5;124m\"\u001b[39m], parent, child, configs, attacker\u001b[38;5;241m=\u001b[39mattacker, model\u001b[38;5;241m=\u001b[39mmodel,save_dir\u001b[38;5;241m=\u001b[39msave_dir\n\u001b[0;32m    100\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_modules.py:154\u001b[0m, in \u001b[0;36mchild_attacking\u001b[1;34m(child_df_with_cluster, child_domain_dict, parent_name, child_name, configs, attacker, model, save_dir)\u001b[0m\n\u001b[0;32m    146\u001b[0m child_model_params \u001b[38;5;241m=\u001b[39m get_model_params(\n\u001b[0;32m    147\u001b[0m     {\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffusion\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiffusion\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    150\u001b[0m     }\n\u001b[0;32m    151\u001b[0m )\n\u001b[0;32m    152\u001b[0m child_T_dict \u001b[38;5;241m=\u001b[39m get_T_dict()\n\u001b[1;32m--> 154\u001b[0m \u001b[43mattack_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_df_with_cluster\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_model_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchild_T_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiffusion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miterations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiffusion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiffusion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_timesteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattacker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattacker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\pipeline_utils.py:686\u001b[0m, in \u001b[0;36mattack_model\u001b[1;34m(df, df_info, model_params, T_dict, steps, batch_size, num_timesteps, device, attacker, model, save_dir)\u001b[0m\n\u001b[0;32m    676\u001b[0m diffusion\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    678\u001b[0m attacker \u001b[38;5;241m=\u001b[39m Attacker(\n\u001b[0;32m    679\u001b[0m     diffusion,\n\u001b[0;32m    680\u001b[0m     train_loader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    684\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39msave_dir,\n\u001b[0;32m    685\u001b[0m )\n\u001b[1;32m--> 686\u001b[0m \u001b[43mattacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\scripts\\attack.py:30\u001b[0m, in \u001b[0;36mAttacker.run_attack\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m step \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps:\n\u001b[0;32m     29\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_iter)\n\u001b[1;32m---> 30\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattacker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattacker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# model_out = self._denoise_fn(x_in, t)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m400\u001b[39m, \u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m800\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m1200\u001b[39m, \u001b[38;5;241m1400\u001b[39m, \u001b[38;5;241m1600\u001b[39m, \u001b[38;5;241m1800\u001b[39m, \u001b[38;5;241m1999\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\gaussian_multinomial_diffsuion.py:758\u001b[0m, in \u001b[0;36mGaussianMultinomialDiffusion.attack\u001b[1;34m(self, x, attacker)\u001b[0m\n\u001b[0;32m    755\u001b[0m x_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_num_t, log_x_cat_t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    757\u001b[0m \u001b[38;5;66;03m# distance = attacker(x)\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m distance \u001b[38;5;241m=\u001b[39m \u001b[43mattacker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distance\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\attack\\components.py:82\u001b[0m, in \u001b[0;36mDDIMAttacker.__call__\u001b[1;34m(self, x0, condition)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x0, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 82\u001b[0m     intermediates, intermediates_denoise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_reverse_and_denoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance(intermediates, intermediates_denoise)\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\attack\\components.py:77\u001b[0m, in \u001b[0;36mDDIMAttacker.get_reverse_and_denoise\u001b[1;34m(self, x0, condition, step)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_reverse_and_denoise\u001b[39m(\u001b[38;5;28mself\u001b[39m, x0, condition, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     76\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize(x0)\n\u001b[1;32m---> 77\u001b[0m     intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_reverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     intermediates_denoise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mddim_denoise(x0, intermediates, condition)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(intermediates), torch\u001b[38;5;241m.\u001b[39mstack(intermediates_denoise)\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\attack\\components.py:112\u001b[0m, in \u001b[0;36mPIA.ddim_reverse\u001b[1;34m(self, x0, condition)\u001b[0m\n\u001b[0;32m    110\u001b[0m intermediates \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    111\u001b[0m terminal_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterval \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattack_num\n\u001b[1;32m--> 112\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps_getter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, terminal_step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterval)):\n\u001b[0;32m    114\u001b[0m     intermediates\u001b[38;5;241m.\u001b[39mappend(eps)\n",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mEpsGetter.__call__\u001b[1;34m(self, xt, condition, noise_level, t)\u001b[0m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m     14\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones([xt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], device\u001b[38;5;241m=\u001b[39mxt\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;241m*\u001b[39m t\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\modules.py:469\u001b[0m, in \u001b[0;36mMLPDiffusion.forward\u001b[1;34m(self, x, timesteps, y)\u001b[0m\n\u001b[0;32m    467\u001b[0m     emb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_emb(y))\n\u001b[0;32m    468\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x) \u001b[38;5;241m+\u001b[39m emb\n\u001b[1;32m--> 469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\modules.py:249\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    247\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 249\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ksush\\attacks\\MIDSTModels\\midst_models\\single_table_TabDDPM\\tab_ddpm\\modules.py:166\u001b[0m, in \u001b[0;36mMLP.Block.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def process_tabddpm_folders(base_path, attacker_name=\"PIA\", interval=10, attack_num=1):\n",
    "    \"\"\"\n",
    "    Processes all 'tabddpm_{n}' folders inside the given base path.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base directory containing 'tabddpm_{n}' folders.\n",
    "    \"\"\"\n",
    "    # Find all folders matching the pattern 'tabddpm_{n}'\n",
    "    tabddpm_folders = glob.glob(os.path.join(base_path, \"tabddpm_*\"))\n",
    "\n",
    "    for path in tabddpm_folders:\n",
    "        print(f\"\\nProcessing folder: {path}\")\n",
    "\n",
    "        # Load pretrained model\n",
    "        model = tabddpm_whitebox_load_pretrained(path)\n",
    "        model = model[(None, 'trans')]['diffusion']._denoise_fn\n",
    "\n",
    "        # Load config\n",
    "        config_path = os.path.join(path, \"trans.json\")\n",
    "        configs, save_dir = load_configs(config_path)\n",
    "\n",
    "        # Display config\n",
    "        print(\"\\n===== Configurations =====\")\n",
    "        print(json.dumps(configs, indent=4))\n",
    "\n",
    "        # Load dataset\n",
    "        tables, relation_order, dataset_meta = load_multi_table(path, train_data=\"challenge\")\n",
    "\n",
    "        # Display table keys\n",
    "        print(\"\\n===== Table Keys =====\")\n",
    "        print(list(tables.keys()))\n",
    "\n",
    "        # Display clustering parameters\n",
    "        params_clustering = configs[\"clustering\"]\n",
    "        print(\"\\n===== Clustering Parameters =====\")\n",
    "        for key, val in params_clustering.items():\n",
    "            print(f\"{key}: {val}\")\n",
    "\n",
    "        # Perform clustering\n",
    "        tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "            tables, relation_order, save_dir, configs\n",
    "        )\n",
    "\n",
    "        print(f\"Finished processing {path}\\n\" + \"=\" * 50)\n",
    "        attacker = ATTACKERS[attacker_name](\n",
    "            torch.from_numpy(np.linspace(FLAGS.beta_1, FLAGS.beta_T, FLAGS.T)).to(DEVICE), interval, attack_num, EpsGetter(model), lambda x: x * 2 - 1, lp=4)\n",
    "\n",
    "        # Launch training from scratch\n",
    "        clava_attacking(tables, relation_order, path, configs, attacker, model)\n",
    "\n",
    "        # # Generate synthetic data from scratch\n",
    "        # clava_synthesizing(\n",
    "        #     tables,\n",
    "        #     relation_order,\n",
    "        #     save_dir,\n",
    "        #     all_group_lengths_prob_dicts,\n",
    "        #     model,\n",
    "        #     configs,\n",
    "        #     # sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    "        #     sample_scale=1.0,\n",
    "        #     save_distances=path,\n",
    "        #     attacker=attacker\n",
    "        # )\n",
    "\n",
    "        print(\"Finished performing the MIA\")\n",
    "\n",
    "\n",
    "# Define base directories\n",
    "dev_base = r\"C:\\Users\\ksush\\attacks\\MIDSTModels\\tabddpm_white_box\\dev\"\n",
    "final_base = r\"C:\\Users\\ksush\\attacks\\MIDSTModels\\tabddpm_white_box\\final\"\n",
    "\n",
    "# Process all tabddpm folders in dev and final\n",
    "process_tabddpm_folders(dev_base)\n",
    "process_tabddpm_folders(final_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save MIA outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryROC\n",
    "\n",
    "# Distance function\n",
    "def distance(x0, x1, lp=2):\n",
    "    return ((x0 - x1).abs()**lp).flatten(1).sum(dim=-1)\n",
    "\n",
    "# Normalize distances to [0,1] range for probability scores\n",
    "def normalize_scores(member_distances, nonmember_distances):\n",
    "    max_dist = max(member_distances.max().item(), nonmember_distances.max().item())\n",
    "    member_probs = 1 - (member_distances / max_dist)\n",
    "    nonmember_probs = 1 - (nonmember_distances / max_dist)\n",
    "    return member_probs, nonmember_probs\n",
    "\n",
    "def process_saved_outputs(base_path):\n",
    "    \"\"\"\n",
    "    Processes 'saved_outputs_0_200_1000.pth' in each 'tabddpm_{n}' folder inside the given base path,\n",
    "    calculates AUROC, TPR at 10% FPR, and saves the predictions in 'predictions.csv'.\n",
    "    \n",
    "    Args:\n",
    "        base_path (str): The base directory containing 'tabddpm_{n}' folders.\n",
    "    \"\"\"\n",
    "    # Find all folders matching the pattern 'tabddpm_{n}'\n",
    "    tabddpm_folders = glob.glob(os.path.join(base_path, \"tabddpm_*\"))\n",
    "\n",
    "    for path in tabddpm_folders:\n",
    "        print(f\"\\nProcessing folder: {path}\")\n",
    "\n",
    "        # Load saved_outputs from the folder\n",
    "        saved_outputs_path = os.path.join(path, \"saved_outputs_0_200_1000.pth\")\n",
    "        if not os.path.exists(saved_outputs_path):\n",
    "            print(f\"Skipped {path} - saved_outputs file not found.\")\n",
    "            continue\n",
    "        \n",
    "        saved_outputs = torch.load(saved_outputs_path)\n",
    "\n",
    "        # Check for challenge labels\n",
    "        label_path = os.path.join(path, \"challenge_label.csv\")\n",
    "        if os.path.exists(label_path):\n",
    "            label_df = pd.read_csv(label_path)\n",
    "            labels = label_df.iloc[:, 0].values  # Convert to numpy array\n",
    "            members = labels == 1\n",
    "            non_members = labels == 0\n",
    "\n",
    "            # Compute distances for T = 0 vs 1000 and T = 0 vs 200\n",
    "            distances_0_1000 = distance(saved_outputs[0], saved_outputs[1000])\n",
    "\n",
    "            distances_0_200 = distance(saved_outputs[0], saved_outputs[200])\n",
    "\n",
    "            # Normalize distances and compute scores\n",
    "            member_scores, nonmember_scores = normalize_scores(distances_0_200[members], distances_0_200[non_members])\n",
    "\n",
    "            # Create labels (1 for members, 0 for non-members)\n",
    "            scores = torch.cat([member_scores, nonmember_scores])\n",
    "            labels = torch.cat([torch.ones_like(member_scores), torch.zeros_like(nonmember_scores)]).long()\n",
    "\n",
    "            # Compute predictions\n",
    "            predictions = (scores).long()\n",
    "\n",
    "            # Save predictions and scores to CSV\n",
    "            predictions_df = pd.DataFrame({'score': scores.numpy()})\n",
    "            predictions_df.to_csv(os.path.join(path, \"predictions.csv\"), index=False, header=False)\n",
    "\n",
    "            # Compute AUROC\n",
    "            auroc = BinaryAUROC()(scores, labels).item()\n",
    "\n",
    "            # Compute ROC curve\n",
    "            roc = BinaryROC()(scores, labels)\n",
    "            fpr, tpr, _ = roc  # Unpack outputs\n",
    "\n",
    "            # Extract TPR at 10% FPR\n",
    "            fpr_threshold = 0.1\n",
    "            tpr_at_1_fpr = tpr[(fpr < fpr_threshold).sum() - 1].item()\n",
    "\n",
    "            # Print results\n",
    "            print(f\"Folder: {path}\")\n",
    "            print(f\"AUROC: {auroc:.4f}\")\n",
    "            print(f\"TPR at 10% FPR: {tpr_at_1_fpr:.4f}\")\n",
    "            print(f\"Predictions saved to {os.path.join(path, 'predictions.csv')}\")\n",
    "        else:\n",
    "            # If challenge_label.csv is not found, just save the predictions based on the available outputs\n",
    "            print(f\"No labels found for {path}, saving predictions without AUROC.\")\n",
    "            distances_0_200 = distance(saved_outputs[0], saved_outputs[200])\n",
    "            # print(f\"Distances 0 vs 1000: {distances_0_200.shape} from {saved_outputs[0].shape} and {saved_outputs[200].shape}\")\n",
    "\n",
    "            # Normalize and save predictions\n",
    "            member_scores, nonmember_scores = normalize_scores(distances_0_200, distances_0_200)  # No label distinction\n",
    "\n",
    "            # Save predictions to CSV\n",
    "            scores = torch.cat([member_scores])\n",
    "            predictions_df = pd.DataFrame({'score': scores.numpy()})\n",
    "            predictions_df.to_csv(os.path.join(path, \"predictions.csv\"), index=False, header=False)\n",
    "\n",
    "            print(f\"Predictions [{predictions_df.shape}] saved to {os.path.join(path, 'predictions.csv')}\")\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Define base directories for dev and final\n",
    "dev_base = r\"C:\\Users\\ksush\\attacks\\MIDSTModels\\tabddpm_white_box\\dev\"\n",
    "final_base = r\"C:\\Users\\ksush\\attacks\\MIDSTModels\\tabddpm_white_box\\final\"\n",
    "\n",
    "# Process all saved_outputs in dev and final\n",
    "process_saved_outputs(dev_base)\n",
    "process_saved_outputs(final_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "saved_outputs = torch.load(\"saved_outputs_0_200_1000.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample distance function\n",
    "def distance(x0, x1, lp=2):\n",
    "    return ((x0 - x1).abs()**lp).flatten(1).sum(dim=-1)\n",
    "\n",
    "\n",
    "# Graph A: Amount of noise predicted per point at different values of T\n",
    "x_values = sorted(saved_outputs.keys())\n",
    "y_values = [saved_outputs[t].abs().mean(dim=1).mean().item() for t in x_values]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_values, y_values, marker='o', linestyle='-', label='Avg. Noise per Point')\n",
    "plt.xlabel(\"T values\")\n",
    "plt.ylabel(\"Average Noise\")\n",
    "plt.title(\"Noise Predicted per Point across Different T Values\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Graph B: Distance between T=0 & T=1000 and T=0 & T=200\n",
    "distances_0_1999 = distance(saved_outputs[0], saved_outputs[1999])\n",
    "distances_0_1800= distance(saved_outputs[0], saved_outputs[1800])\n",
    "distances_0_1600 = distance(saved_outputs[0], saved_outputs[1600])\n",
    "distances_0_1400 = distance(saved_outputs[0], saved_outputs[1400])\n",
    "distances_0_1200 = distance(saved_outputs[0], saved_outputs[1200])\n",
    "distances_0_1000 = distance(saved_outputs[0], saved_outputs[1000])\n",
    "distances_0_800 = distance(saved_outputs[0], saved_outputs[800])\n",
    "distances_0_600 = distance(saved_outputs[0], saved_outputs[600])\n",
    "distances_0_400 = distance(saved_outputs[0], saved_outputs[400])\n",
    "distances_0_200 = distance(saved_outputs[0], saved_outputs[200])\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(distances_0_1999.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=1999)')\n",
    "plt.hist(distances_0_200.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=200)')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Distances between Noise Predictions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# TODO: repeat for other values of T\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(distances_0_1800.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=1800)')\n",
    "plt.hist(distances_0_200.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=200)')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Distances between Noise Predictions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(distances_0_1600.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=1600)')\n",
    "plt.hist(distances_0_200.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=200)')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Distances between Noise Predictions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(distances_0_1400.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=1400)')\n",
    "plt.hist(distances_0_200.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=200)')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Distances between Noise Predictions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(distances_0_1000.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=1000)')\n",
    "plt.hist(distances_0_200.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=200)')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Distances between Noise Predictions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(distances_0_800.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=800)')\n",
    "plt.hist(distances_0_200.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=200)')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Distances between Noise Predictions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(distances_0_600.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=600)')\n",
    "plt.hist(distances_0_200.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=200)')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Distances between Noise Predictions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(distances_0_400.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=400)')\n",
    "plt.hist(distances_0_200.numpy(), bins=50, alpha=0.6, label='Dist(T=0, T=200)')\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Distances between Noise Predictions\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "saved_outputs = torch.load(\"saved_outputs_0_200_1000.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Distance function\n",
    "def distance(x0, x1, lp=2):\n",
    "    return ((x0 - x1).abs()**lp).flatten(1).sum(dim=-1)\n",
    "label_df = pd.read_csv(\"tabddpm_white_box/train/tabddpm_1/challenge_label.csv\")\n",
    "\n",
    "# Assume saved_outputs is your dictionary with noise tensors\n",
    "# Assume label_df is a DataFrame with one column containing 1 (member) or 0 (non-member)\n",
    "labels = label_df.iloc[:, 0].values  # Convert to numpy array\n",
    "\n",
    "# Separate members and non-members\n",
    "members = labels == 1\n",
    "non_members = labels == 0\n",
    "\n",
    "# Plot noise magnitude per point for different T values\n",
    "plt.figure(figsize=(10, 6))\n",
    "for T, noise in saved_outputs.items():\n",
    "    noise_magnitude = noise.norm(dim=1)  # Compute magnitude of noise per point\n",
    "    plt.plot([T] * len(noise_magnitude[members]), noise_magnitude[members], 'bo', alpha=0.02, label='Members' if T == 0 else \"\")\n",
    "    plt.plot([T] * len(noise_magnitude[non_members]), noise_magnitude[non_members], 'ro', alpha=0.02, label='Non-Members' if T == 0 else \"\")\n",
    "\n",
    "plt.xlabel(\"T values\")\n",
    "plt.ylabel(\"Noise Magnitude\")\n",
    "plt.title(\"Noise Magnitude per Point Over Different T values\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute distances for T = 0 vs 1000 and T = 0 vs 200\n",
    "distances_0_1999 = distance(saved_outputs[0], saved_outputs[1999])\n",
    "distances_0_1800= distance(saved_outputs[0], saved_outputs[1800])\n",
    "distances_0_1600 = distance(saved_outputs[0], saved_outputs[1600])\n",
    "distances_0_1400 = distance(saved_outputs[0], saved_outputs[1400])\n",
    "distances_0_1200 = distance(saved_outputs[0], saved_outputs[1200])\n",
    "distances_0_1000 = distance(saved_outputs[0], saved_outputs[1000])\n",
    "distances_0_800 = distance(saved_outputs[0], saved_outputs[800])\n",
    "distances_0_600 = distance(saved_outputs[0], saved_outputs[600])\n",
    "distances_0_400 = distance(saved_outputs[0], saved_outputs[400])\n",
    "distances_0_200 = distance(saved_outputs[0], saved_outputs[200])\n",
    "\n",
    "# Separate distances for members and non-members\n",
    "dist_0_1000_members = distances_0_1000[members]\n",
    "dist_0_1000_non_members = distances_0_1000[non_members]\n",
    "\n",
    "dist_0_200_members = distances_0_400[members]\n",
    "dist_0_200_non_members = distances_0_400[non_members]\n",
    "\n",
    "# Plot histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(dist_0_1000_members, bins=50, alpha=0.6, color='b', label=\"Members\")\n",
    "axes[0].hist(dist_0_1000_non_members, bins=50, alpha=0.6, color='r', label=\"Non-Members\")\n",
    "axes[0].set_title(\"Distance between T=0 and T=1999\")\n",
    "axes[0].set_xlabel(\"Distance\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(dist_0_200_members, bins=50, alpha=0.6, color='b', label=\"Members\")\n",
    "axes[1].hist(dist_0_200_non_members, bins=50, alpha=0.6, color='r', label=\"Non-Members\")\n",
    "axes[1].set_title(\"Distance between T=0 and T=400\")\n",
    "axes[1].set_xlabel(\"Distance\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torchmetrics.classification import BinaryAUROC, BinaryROC\n",
    "\n",
    "# Normalize distances to [0,1] range for probability scores\n",
    "def normalize_scores(member_distances, nonmember_distances):\n",
    "    max_dist = max(member_distances.max().item(), nonmember_distances.max().item())\n",
    "    member_probs = 1 - (member_distances / max_dist)\n",
    "    nonmember_probs = 1 - (nonmember_distances / max_dist)\n",
    "    return member_probs, nonmember_probs\n",
    "\n",
    "# Compute scores\n",
    "member_scores, nonmember_scores = normalize_scores(distances_0_400[members], distances_0_400[non_members])\n",
    "\n",
    "# Create labels (1 for members, 0 for non-members)\n",
    "labels = torch.cat([torch.ones_like(member_scores), torch.zeros_like(nonmember_scores)]).long()\n",
    "scores = torch.cat([member_scores, nonmember_scores])\n",
    "\n",
    "# Compute predictions\n",
    "predictions = (scores).long()\n",
    "\n",
    "# Save predictions and scores to CSV\n",
    "df = pd.DataFrame({\n",
    "    'score': scores.numpy(),\n",
    "})\n",
    "df.to_csv(os.path.join(path, \"predictions.csv\"), index=False, header=False)\n",
    "\n",
    "# Compute AUROC\n",
    "auroc = BinaryAUROC()(scores, labels).item()\n",
    "\n",
    "# Compute ROC curve\n",
    "roc = BinaryROC()(scores, labels)\n",
    "fpr, tpr, _ = roc  # Unpack outputs\n",
    "\n",
    "# Extract TPR at 10% FPR\n",
    "fpr_threshold = 0.1\n",
    "tpr_at_1_fpr = tpr[(fpr < fpr_threshold).sum() - 1].item()\n",
    "\n",
    "# Print results\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(f\"TPR at 10% FPR: {tpr_at_1_fpr:.4f}\")\n",
    "print(\"Predictions saved to predictions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ5My1aIV2Tv"
   },
   "source": [
    "## Package Imports and Evironment Setup\n",
    "\n",
    "Ensure that you have installed the proper dependenices to run the notebook. The environment installation instructions are available [here](https://github.com/VectorInstitute/MIDSTModels/tree/main/starter_kits). Now that we have verfied we have the proper packages installed, lets import them and define global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MB3iIVMTFYyB"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from data import get_challenge_points, get_challenge_labels\n",
    "from metrics import get_tpr_at_fpr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MB3iIVMTFYyB"
   },
   "outputs": [],
   "source": [
    "TABDDPM_DATA_DIR = \"tabddpm_white_box\"\n",
    "TABSYN_DATA_DIR = \"tabsyn_white_box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack(base_dir, attacker_name=\"PIA\", attack_num=30, interval=10, lp=3):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # logger.addHandler(RichHandler())\n",
    "\n",
    "    logger.info(\"loading the attacked model...\")\n",
    "\n",
    "    # Initialize attacker\n",
    "    phases = [\"train\"]\n",
    "    # phases = [\"dev\", \"final\"]\n",
    "    \n",
    "    logger.info(\"attack start...\")\n",
    "    for phase in phases:\n",
    "        root = os.path.join(base_dir, phase)\n",
    "        for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            if \"30\" not in str(path):\n",
    "                continue\n",
    "\n",
    "            model = tabddpm_whitebox_load_pretrained(path)\n",
    "            attacker = ATTACKERS[attacker_name](\n",
    "                torch.from_numpy(np.linspace(FLAGS.beta_1, FLAGS.beta_T, FLAGS.T)).to(DEVICE), interval, attack_num, EpsGetter(model), lp=lp)\n",
    "\n",
    "            challenge_points = get_challenge_points(path)\n",
    "            challenge_labels = get_challenge_labels(path)\n",
    "            raw_predictions = torch.stack([attacker(cp.to(DEVICE).float()) for cp in challenge_points])\n",
    "            raw_predictions_means = raw_predictions.mean(dim=1).cpu().detach().numpy()\n",
    "            challenge_labels_np = challenge_labels.values.squeeze()\n",
    "\n",
    "            non_member_distances = raw_predictions_means[challenge_labels_np == 0]\n",
    "            member_distances = raw_predictions_means[challenge_labels_np == 1]\n",
    "\n",
    "            # Create figure with three subplots\n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 15), height_ratios=[1, 1, 1.5])\n",
    "            \n",
    "            # Plot 1: Full distribution\n",
    "            bins = np.linspace(min(raw_predictions_means), max(raw_predictions_means), 30)\n",
    "            ax1.hist(non_member_distances, bins=bins, alpha=0.6, \n",
    "                    label=f'Non-member (n={len(non_member_distances)})', \n",
    "                    color='blue')\n",
    "            ax1.hist(member_distances, bins=bins, alpha=0.6,\n",
    "                    label=f'Member (n={len(member_distances)})', \n",
    "                    color='red')\n",
    "            \n",
    "            ax1.set_xlabel('Distance')\n",
    "            ax1.set_ylabel('Count')\n",
    "            ax1.set_title('Full Distribution of Distances')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            # First zoom: Find the densest region\n",
    "            all_data = np.concatenate([member_distances, non_member_distances])\n",
    "            Q1 = np.percentile(all_data, 25)\n",
    "            Q3 = np.percentile(all_data, 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 0.5 * IQR\n",
    "            upper_bound = Q3 + 0.5 * IQR\n",
    "\n",
    "            # Filter data for first zoom\n",
    "            non_member_filtered = non_member_distances[\n",
    "                (non_member_distances >= lower_bound) & \n",
    "                (non_member_distances <= upper_bound)\n",
    "            ]\n",
    "            member_filtered = member_distances[\n",
    "                (member_distances >= lower_bound) & \n",
    "                (member_distances <= upper_bound)\n",
    "            ]\n",
    "\n",
    "            # Plot 2: First zoom level\n",
    "            detailed_bins = np.linspace(lower_bound, upper_bound, 50)\n",
    "            ax2.hist(non_member_filtered, bins=detailed_bins, alpha=0.6,\n",
    "                    label=f'Non-member (n={len(non_member_filtered)})', \n",
    "                    color='blue')\n",
    "            ax2.hist(member_filtered, bins=detailed_bins, alpha=0.6,\n",
    "                    label=f'Member (n={len(member_filtered)})', \n",
    "                    color='red')\n",
    "            \n",
    "            ax2.set_xlabel('Distance')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.set_title('First Zoom Level\\n'\n",
    "                        f'(Range: {lower_bound:.2f} to {upper_bound:.2f})')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            # Second zoom: Find the even denser region\n",
    "            filtered_data = np.concatenate([member_filtered, non_member_filtered])\n",
    "            Q1_filtered = np.percentile(filtered_data, 25)\n",
    "            Q3_filtered = np.percentile(filtered_data, 75)\n",
    "            IQR_filtered = Q3_filtered - Q1_filtered\n",
    "            lower_bound_filtered = Q1_filtered - 0.25 * IQR_filtered  # Using tighter bounds\n",
    "            upper_bound_filtered = Q3_filtered + 0.25 * IQR_filtered\n",
    "\n",
    "            # Filter data for second zoom\n",
    "            non_member_filtered_2 = non_member_filtered[\n",
    "                (non_member_filtered >= lower_bound_filtered) & \n",
    "                (non_member_filtered <= upper_bound_filtered)\n",
    "            ]\n",
    "            member_filtered_2 = member_filtered[\n",
    "                (member_filtered >= lower_bound_filtered) & \n",
    "                (member_filtered <= upper_bound_filtered)\n",
    "            ]\n",
    "\n",
    "            # Plot 3: Second zoom level with very fine bins\n",
    "            very_detailed_bins = np.linspace(lower_bound_filtered, upper_bound_filtered, 100)\n",
    "            ax3.hist(non_member_filtered_2, bins=very_detailed_bins, alpha=0.6,\n",
    "                    label=f'Non-member (n={len(non_member_filtered_2)})', \n",
    "                    color='blue')\n",
    "            ax3.hist(member_filtered_2, bins=very_detailed_bins, alpha=0.6,\n",
    "                    label=f'Member (n={len(member_filtered_2)})', \n",
    "                    color='red')\n",
    "            \n",
    "            ax3.set_xlabel('Distance')\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.set_title('Second Zoom Level (Finest Detail)\\n'\n",
    "                        f'(Range: {lower_bound_filtered:.2f} to {upper_bound_filtered:.2f})')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "\n",
    "            # Add statistical information for the finest zoom level\n",
    "            stats_text = (\n",
    "                f'Dense Region Stats:\\n'\n",
    "                f'Non-member:\\n'\n",
    "                f'  Mean: {np.mean(non_member_filtered_2):.3f}\\n'\n",
    "                f'  Std: {np.std(non_member_filtered_2):.3f}\\n'\n",
    "                f'Member:\\n'\n",
    "                f'  Mean: {np.mean(member_filtered_2):.3f}\\n'\n",
    "                f'  Std: {np.std(member_filtered_2):.3f}'\n",
    "            )\n",
    "            ax3.text(0.02, 0.98, stats_text, \n",
    "                    transform=ax3.transAxes,\n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(path, f\"distance_distribution_T={interval}_lp={lp}.png\"), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "            # Save detailed statistics for all zoom levels\n",
    "            stats_summary = pd.DataFrame({\n",
    "                'Metric': ['Full Mean', 'Full Std', \n",
    "                        'First Zoom Mean', 'First Zoom Std',\n",
    "                        'Second Zoom Mean', 'Second Zoom Std',\n",
    "                        'Points in Densest Region', 'Total Points'],\n",
    "                'Member': [\n",
    "                    member_distances.mean(),\n",
    "                    member_distances.std(),\n",
    "                    member_filtered.mean(),\n",
    "                    member_filtered.std(),\n",
    "                    member_filtered_2.mean(),\n",
    "                    member_filtered_2.std(),\n",
    "                    len(member_filtered_2),\n",
    "                    len(member_distances)\n",
    "                ],\n",
    "                'Non-member': [\n",
    "                    non_member_distances.mean(),\n",
    "                    non_member_distances.std(),\n",
    "                    non_member_filtered.mean(),\n",
    "                    non_member_filtered.std(),\n",
    "                    non_member_filtered_2.mean(),\n",
    "                    non_member_filtered_2.std(),\n",
    "                    len(non_member_filtered_2),\n",
    "                    len(non_member_distances)\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            stats_summary.to_csv(os.path.join(path, f\"detailed_statistics_T={interval}_lp={lp}.csv\"), index=False)\n",
    "\n",
    "            # Continue with the original prediction code\n",
    "            normalized_preds = []\n",
    "            for pred_batch in raw_predictions:\n",
    "                binary_preds = (pred_batch <= 1000000).float()\n",
    "                normalized_preds.append(binary_preds)\n",
    "\n",
    "            final_predictions = torch.stack(normalized_preds)\n",
    "            predictions_cpu = final_predictions.cpu().detach().numpy()\n",
    "\n",
    "            with open(os.path.join(path, \"prediction.csv\"), mode=\"w\", newline=\"\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                for value in predictions_cpu.squeeze():\n",
    "                    writer.writerow([value])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack(base_dir=\"tabddpm_white_box\",\n",
    "        attacker_name=\"PIAN\",\n",
    "        attack_num=3,\n",
    "        interval=200,\n",
    "        lp=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGhGsrlPV2Ty"
   },
   "source": [
    "## Scoring\n",
    "\n",
    "Let's see how the attack does on `train`, for which we have the ground truth.\n",
    "When preparing a submission, you can use part of `train` to develop an attack and a held-out part to evaluate your attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-UN3zfuPV2Ty"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "def safe_load_data(filepath: str, is_prediction: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Safely load data from CSV files with error handling and debugging.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        is_prediction: Whether this is a prediction file (True) or label file (False)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Loaded and validated data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_prediction:\n",
    "            # Read the file line by line and parse each array\n",
    "            predictions = []\n",
    "            with open(filepath, 'r') as f:\n",
    "                for line in f:\n",
    "                    # Remove brackets and split by spaces\n",
    "                    clean_line = line.strip().replace('[', '').replace(']', '')\n",
    "                    # Convert space-separated strings to floats\n",
    "                    row = np.array([float(x) for x in clean_line.split()])\n",
    "                    predictions.append(row)\n",
    "            return np.array(predictions)\n",
    "        else:\n",
    "            # For label files, skip header and use numpy\n",
    "            return np.loadtxt(filepath, skiprows=1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {str(e)}\")\n",
    "        print(f\"File contents (first few lines):\")\n",
    "        with open(filepath, 'r') as f:\n",
    "            print(f.read(500))\n",
    "        raise\n",
    "\n",
    "def get_tpr_at_fpr(y_true: np.ndarray, y_score: np.ndarray, target_fpr: float = 0.1) -> float:\n",
    "    \"\"\"Calculate TPR at a specific FPR threshold.\"\"\"\n",
    "    # Input validation\n",
    "    if not isinstance(y_true, np.ndarray) or not isinstance(y_score, np.ndarray):\n",
    "        raise TypeError(\"Inputs must be numpy arrays\")\n",
    "    if y_true.shape != y_score.shape:\n",
    "        raise ValueError(f\"Shape mismatch: y_true {y_true.shape} != y_score {y_score.shape}\")\n",
    "    \n",
    "    # Sort scores and corresponding truth values\n",
    "    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n",
    "    y_score = y_score[desc_score_indices]\n",
    "    y_true = y_true[desc_score_indices]\n",
    "    \n",
    "    n_neg = np.sum(y_true == 0)\n",
    "    n_pos = np.sum(y_true == 1)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if n_neg == 0 or n_pos == 0:\n",
    "        print(\"Warning: Found no positive or negative samples\")\n",
    "        return 0.0\n",
    "        \n",
    "    tpr = np.cumsum(y_true) / n_pos\n",
    "    fpr = np.cumsum(1 - y_true) / n_neg\n",
    "    \n",
    "    for i in range(len(fpr)):\n",
    "        if fpr[i] >= target_fpr:\n",
    "            return tpr[i]\n",
    "    return 1.0\n",
    "\n",
    "def evaluate_membership_inference(base_dirs: List[str]) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate membership inference attack results across multiple directories.\n",
    "    \n",
    "    Args:\n",
    "        base_dirs: List of base directories containing prediction files\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_tpr_at_fpr, best_auc)\n",
    "    \"\"\"\n",
    "    tpr_at_fpr_list = []\n",
    "    auc_list = []\n",
    "    \n",
    "    for base_dir in base_dirs:\n",
    "        predictions = []\n",
    "        solutions = []\n",
    "        root = os.path.join(base_dir, \"train\")\n",
    "        \n",
    "        if not os.path.exists(root):\n",
    "            print(f\"Warning: Directory not found: {root}\")\n",
    "            continue\n",
    "            \n",
    "        model_folders = sorted(os.listdir(root), key=lambda d: int(d.split('_')[1]))\n",
    "        if not model_folders:\n",
    "            print(f\"Warning: No model folders found in {root}\")\n",
    "            continue\n",
    "        \n",
    "        # Load and process all predictions and solutions\n",
    "        for model_folder in model_folders:\n",
    "            path = os.path.join(root, model_folder)\n",
    "            pred_path = os.path.join(path, \"prediction.csv\")\n",
    "            label_path = os.path.join(path, \"challenge_label.csv\")\n",
    "            \n",
    "            if not (os.path.exists(pred_path) and os.path.exists(label_path)):\n",
    "                print(f\"Warning: Missing files in {path}\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Load predictions\n",
    "                pred = safe_load_data(pred_path, is_prediction=True)\n",
    "                predictions.append(pred)\n",
    "                \n",
    "                # Load ground truth\n",
    "                solution = safe_load_data(label_path, is_prediction=False)\n",
    "                if solution.ndim == 0:\n",
    "                    solution = solution.reshape(1)\n",
    "                solutions.append(solution)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing folder {model_folder}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if not predictions or not solutions:\n",
    "            print(f\"Warning: No valid data found in {base_dir}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Concatenate all predictions and solutions\n",
    "            predictions = np.concatenate(predictions)\n",
    "            solutions = np.concatenate(solutions)\n",
    "            \n",
    "            print(f\"\\nData shapes for {os.path.basename(base_dir)}:\")\n",
    "            print(f\"Predictions shape: {predictions.shape}\")\n",
    "            print(f\"Solutions shape: {solutions.shape}\")\n",
    "            \n",
    "            # Calculate metrics for each attacker's predictions\n",
    "            num_attackers = predictions.shape[1]\n",
    "            for attacker_idx in range(num_attackers):\n",
    "                attacker_preds = predictions[:, attacker_idx]\n",
    "                \n",
    "                # Basic data validation\n",
    "                if np.any(np.isnan(attacker_preds)):\n",
    "                    print(f\"Warning: NaN values found in predictions for attacker {attacker_idx}\")\n",
    "                    continue\n",
    "                    \n",
    "                # Calculate metrics\n",
    "                tpr_at_fpr = get_tpr_at_fpr(solutions, attacker_preds)\n",
    "                tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "                \n",
    "                try:\n",
    "                    auc = roc_auc_score(solutions, attacker_preds)\n",
    "                    auc_list.append(auc)\n",
    "                except ValueError as e:\n",
    "                    print(f\"Warning: Could not calculate AUC for attacker {attacker_idx}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                print(f\"{os.path.basename(base_dir)} Attacker {attacker_idx + 1}:\")\n",
    "                print(f\"  TPR at FPR==10%: {tpr_at_fpr:.4f}\")\n",
    "                print(f\"  AUC: {auc:.4f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing directory {base_dir}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Get best scores\n",
    "    final_tpr_at_fpr = max(tpr_at_fpr_list) if tpr_at_fpr_list else 0.0\n",
    "    final_auc = max(auc_list) if auc_list else 0.0\n",
    "    \n",
    "    print(f\"\\nBest scores across all attackers:\")\n",
    "    print(f\"Final Train Attack TPR at FPR==10%: {final_tpr_at_fpr:.4f}\")\n",
    "    print(f\"Final Train Attack AUC: {final_auc:.4f}\")\n",
    "    \n",
    "    return final_tpr_at_fpr, final_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dirs = [TABDDPM_DATA_DIR]\n",
    "final_tpr, final_auc = evaluate_membership_inference(base_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9LZ-EhfV2Ty"
   },
   "source": [
    "## Packaging the submission\n",
    "\n",
    "Now we can store the predictions into a zip file, which you can submit to CodaBench. Importantly, we create a single zip file for dev and final. The structure of the submission is as follows:\n",
    "\n",
    "```\n",
    "└── root_folder\n",
    "    ├── tabsyn_white_box\n",
    "    │   ├── dev\n",
    "    │   │   └── tabsyn_#\n",
    "    │   │       └── prediction.csv\n",
    "    │   └── final\n",
    "    │       └── tabsyn_#\n",
    "    │           └── prediction.csv\n",
    "    └── tabddpm_white_box\n",
    "        ├── dev \n",
    "        │   └── tabddpm_#\n",
    "        │       └── prediction.csv\n",
    "        └── final \n",
    "            └── tabddpm_# \n",
    "                └── prediction.csv\n",
    "```\n",
    "**Note:** The `root_folder` can have any name but it is important all of the subdirectories follow the above structure and naming conventions. \n",
    "\n",
    "If a participant is looking to submit an attack for only one of TabSyn and TabDDPM, they can simply omit the other directory (ie `tabddpm_white_box` or `tabsyn_white_box` from the root_folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd MIDSTModels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ats5N4AoV2Tz"
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_numpy_array(array_str):\n",
    "    \"\"\"Fix NumPy-style array formatting and convert it to a Python list.\"\"\"\n",
    "    array_str = array_str.strip()  # Remove leading/trailing spaces\n",
    "    array_str = re.sub(r'\\s+', ',', array_str)  # Replace spaces with commas\n",
    "    array_str = array_str.replace(\"[,\", \"[\").replace(\",]\", \"]\")  # Fix misplaced commas\n",
    "    return eval(array_str)  # Convert string to list\n",
    "\n",
    "with zipfile.ZipFile(\"white_box_single_table_submission.zip\", 'w') as zipf:\n",
    "    for phase in [\"dev\", \"final\"]:\n",
    "        for base_dir in [TABDDPM_DATA_DIR]:\n",
    "            root = os.path.join(base_dir, phase)\n",
    "            for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):\n",
    "                path = os.path.join(root, model_folder)\n",
    "                if not os.path.isdir(path): \n",
    "                    continue\n",
    "\n",
    "                file = os.path.join(path, \"predictions.csv\")\n",
    "                if os.path.exists(file):\n",
    "                    # Load CSV\n",
    "                    df = pd.read_csv(file, header=None)\n",
    "\n",
    "                    # Convert NumPy-style arrays to proper lists\n",
    "                    df = df[0].apply(lambda x: parse_numpy_array(x) if isinstance(x, str) else x)\n",
    "\n",
    "                    # Compute mean for each row\n",
    "                    df_mean = df.apply(lambda x: np.mean(x) if isinstance(x, list) else x)\n",
    "                    file = os.path.join(path, \"prediction.csv\")\n",
    "                    # Save the new CSV\n",
    "                    df_mean.to_csv(file, index=False, header=False)\n",
    "\n",
    "                    # Add to ZIP\n",
    "                    arcname = os.path.relpath(file, os.path.dirname(base_dir))\n",
    "                    zipf.write(file, arcname=arcname)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"`predictions.csv` not found in {path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated white_box_single_table_submission.zip can be directly submitted to the dev phase in the CodaBench UI. Although this submission contains your predictions for both the dev and final set, you will only receive feedback on your predictions for the dev phase. The predictions for the final phase will be evaluated once the competiton ends using the most recent submission to the dev phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
